services:
  caddy:
    image: caddy:2-alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      - frontend
      - backend

  backend:
    build:
      context: .
      dockerfile: model_training/scripts/Dockerfile
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    volumes:
      - ./data:/app/data
      - ./model_training/vector_store:/app/model_training/vector_store
      - ./model_training/vector_store_examples:/app/model_training/vector_store_examples

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_storage:/root/.ollama
      - ./entrypoint.sh:/entrypoint.sh
    entrypoint: [ "/usr/bin/bash", "/entrypoint.sh" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "5173:5173"
    depends_on:
      - backend

volumes:
  ollama_storage:
  caddy_data:
  caddy_config:
